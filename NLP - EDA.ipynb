{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Workflow(2)-EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text data, EDA is to summarize the main characteristics of the dateset and find some patterns before identifying the hidden patterns with machine learning techniques. For example, for the actors we can look ata the following:\n",
    "1. **Most common words**\n",
    "2. **Size of vocabulary** - look number of unique words and also how quickly someone speaks\n",
    "3. **Amount of profanity**\n",
    "\n",
    "But before we dive into any text EDA, we need to identify what specific questions we need to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read in the document-term matrix\n",
    "data = pd.read_pickle('dtm.pkl')\n",
    "\n",
    "# by using transpose we put the name of actors on the columns, \n",
    "#this will be easier for us to find the top 30 words she/he like to use\n",
    "data = data.transpose()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 30 words by each actors\n",
    "top_dict = {}\n",
    "\n",
    "for c in data.columns:\n",
    "    # after sort the values, return a datafrme (one index, one/more values)\n",
    "    top = data[c].sort_values(ascending = False).head(30)\n",
    "    top_dict[c] = list(zip(top.index, top.values))\n",
    "\n",
    "# top_dict is a dictornary, \n",
    "# the key is the name of actor\n",
    "# the values is a tuple nested list, just like[('a',1),('b',2)]\n",
    "top_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ab', 3), ('cd', 5)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to show list(zip(a,b))\n",
    "a=['ab','cd']\n",
    "value = [3,5]\n",
    "list(zip(a,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 15 words said by each actor\n",
    "for actor, top_words in top_dict.items():\n",
    "    print(actor)\n",
    "    print(','.join([word for word,count in top_words[0:14]]))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some of the top words have little meaning, that means we need to add them to stop words list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# first pull out the top 30 words for each actor\n",
    "words = []\n",
    "for actor in data.columns:\n",
    "    top = [word for (word,count) in top_dict[actor]]\n",
    "    for t in top:\n",
    "        words.append(t)      \n",
    "words\n",
    "\n",
    "# Aggregate the words list and identify most common words\n",
    "Counter(words).most_common()\n",
    "\n",
    "# If more than half of the actors(total 12) have it as top word, exclude it from the list\n",
    "add_stop_words = [word for word,count in Counter(words).most_common() if count > 6]\n",
    "add_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updata document term matrix with the new list of stop words\n",
    "from sklean.feature_extraction import text\n",
    "from sklean.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Read in cleaned data\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "\n",
    "# Add new stop words\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate document-term matrix\n",
    "cv = CountVectorizer(stop_words = stop_words)\n",
    "data_cv = cv.fit_transform(data_clean.transcript)\n",
    "data_dtm_update1 = pd.DataFrame(data_cv.toarray(), columns = cv.get_feature_names())\n",
    "data_dtm_update1.index = data_clean.index\n",
    "\n",
    "# Pickle it for later use\n",
    "import pickle\n",
    "#pickle.dump and *.to_pickle is basically the same method to save file\n",
    "pickle.dump(cv, open('cv_stop.pkl', 'wb'))\n",
    "data_dtm_update1.to_pickle('dtm_stop.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we got the top words for each actor, one of the best way to visually communicate this to someone else is to use **Word Cloud**. By looking into the word could, we can identify if the data make sense. If not, we have be back to the previous step to clean the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wc = WordCloud(stopwords = stop_words, backgroud_color = 'white', max_font_size = 150, random_state = 42)\n",
    "\n",
    "# to help give title to subplot, we can create a full_name list\n",
    "full_names = ['...', '...',...]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create subplots for each actors\n",
    "for index, actor in enumerate(data.columns):\n",
    "    wc.generate(data_clean.transcript[actor])\n",
    "    \n",
    "    # since we have 12 actors, create a subplot 3*4\n",
    "    plt.subplot(3,4, index+1)\n",
    "    plt.imshow(wc, interpolation = 'bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(full_names[index]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Size of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of unique words each actor uses\n",
    "unique_list = []\n",
    "for actor in data.columns:\n",
    "    uniques = data[actor].to_numpy().nonzero()[0].size\n",
    "    unique_list.append(unique)\n",
    "\n",
    "\n",
    "data_words = pd.DataFrame(list(zip(full_names, unique_list)), columns = ['actor', 'unique_words'])\n",
    "data_unique_sort = data_words.sort_values(by='unique_words')\n",
    "data_unique_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the speach speed of each actor\n",
    "\n",
    "# First find the total number of words each actor uses\n",
    "total_list = []\n",
    "for actor in data.columns:\n",
    "    totals = sum(data[actor])\n",
    "    total_list.append(totals)\n",
    "    \n",
    "# Check the run time from IMDB\n",
    "run_times = [**, **, ...]\n",
    "\n",
    "data_words['total_words'] = total_list\n",
    "data_words['run_times'] = run_times\n",
    "data_words['words_per_minute'] = data_words['total_words']/data_words['run_times']\n",
    "\n",
    "# Sort the dataframe by words per minute to see who talks the slowest and fastest\n",
    "data_wpm_sort = data_words.sort_values(by ='words_per_minute')\n",
    "data_wpm_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulalize the plot\n",
    "import numpy as np\n",
    "\n",
    "# np.arange(start=1, stop=10, step=3)\n",
    "y_pos = np.arange(len(data_words))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.barh(y_pos, data_unique_sort, align = 'center')\n",
    "plt.yticks(y_pos, data_unique_sort.actor)\n",
    "plt.title('Number of Unique Words', fontsize = 20)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.barh(y_pos, data_wpm_sort.words_per_minute, align = 'center')\n",
    "plt.yticks(y_pos, data_wpm_sort.actor)\n",
    "plt.title('Number of Words Per Minute', fontsize = 20)\n",
    "\n",
    "plt.tight_layout() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Amount of Profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's isolate just these bad words\n",
    "data_bad_words = data.transpose()[['fucking', 'fuck', 'shit']]\n",
    "data_profanity = pd.concat([data_bad_words.fucking + data_bad_words.fuck, data_bad_words.shit], axis=1)\n",
    "data_profanity.columns = ['f_word', 's_word']\n",
    "data_profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a scatter plot of our findings\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "for i, comedian in enumerate(data_profanity.index):\n",
    "    x = data_profanity.f_word.loc[comedian]\n",
    "    y = data_profanity.s_word.loc[comedian]\n",
    "    plt.scatter(x, y, color='blue')\n",
    "    plt.text(x+1.5, y+0.5, full_names[i], fontsize=10)\n",
    "    plt.xlim(-5, 155) \n",
    "    \n",
    "plt.title('Number of Bad Words Used in Routine', fontsize=20)\n",
    "plt.xlabel('Number of F Bombs', fontsize=15)\n",
    "plt.ylabel('Number of S Words', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
