Bag of Words(BOW):
Bag of words is a technique for feature extration from text. 
Bag of words model processes the text to find how many times each word appeared in the sentence. This is also called as vectorization.

Steps for creating BOW:
. Tokenize the text into sentences
. Tokenize sentences into words
. Remove punctuation or stop words
. Convert the words to lower text
. Create the frequency distribution of words

Problem with Bag of words
In the bag of words model, each document is represented as a word-count vector. The size of the vector is equal to the number of elements in the vocabulary. If most of the elements are zero then the bag of words will be a sparse matrx. Also Bag of words does not take into consideration the order in which they appear.


Word Embedding
Word Embeddings is a technique where individual words of a domain are represented as real-valued vectors in a lower dimentsional space, which solved the problem of BOW.

Word embedding are considered to be one of the successful applications of unsupervised learning at present. Embeddings use a lower-dimentionsal space while preserving semantic relationships.

One Hot Encoding, TF-IDF, Word2Vec, FastText are frequently used Word Embedding methods.
1. One Hot Encoding
2. TF-IDF: statistical measure used to determine the mathematical significance of words in documents. The TF-IDF value is obtained by multiplying TF and IDF values. 
    TF: Term Frequency is the ratio of the number of target terms in the document to the total number of terms in the document.
    IDF: Inverse Document Frequencey is the logarithm of the ratio of the total number of documents to the number of documents in which the target term occurs.
3. Word2Vec(GloVe): unsupervised learning process. Unlabeled data is trained via artifical neural networks to create the word2vec model that generates word vectors.
    . CBOW(continuous bag of words) algorithm
    . skip-gram algorithm
4. FastText(Subword Model): working logic is similar to word2vec, the biggest difference is that is uses N-grams of words during training. In subword model, words with the same roots do share parameters.
    

references:
https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4

https://towardsdatascience.com/word-embedding-techniques-word2vec-and-tf-idf-explained-c5d02e34d08#:~:text=The%20word%20embedding%20techniques%20are,purpose%20of%20processing%20the%20data.
